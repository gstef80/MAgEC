{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIMICIII Mechanical Ventilation MAgECs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os \n",
    "import random\n",
    "import datetime\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(22891)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information used to create a database connection\n",
    "sqluser = 'postgres'\n",
    "dbname = 'mimic'\n",
    "schema_name = 'mimiciii'\n",
    "\n",
    "engine = create_engine(\"postgresql+psycopg2://{}:{}@/{}\".format(sqluser, sqluser, dbname))\n",
    "\n",
    "schema_name = 'mimiciii'\n",
    "conn = engine.connect()\n",
    "conn.execute('SET search_path to ' + schema_name)\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM mimic_users_study;\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals = ['heartrate_mean', 'sysbp_mean', 'diasbp_mean', 'meanbp_mean',\n",
    "          'resprate_mean', 'tempc_mean', 'spo2_mean', 'glucose_mean']\n",
    "labs = ['aniongap', 'albumin', 'bicarbonate', 'bilirubin', 'creatinine', \n",
    "        'chloride', 'glucose', 'hemoglobin', 'lactate', \n",
    "        'magnesium', 'phosphate', 'platelet', 'potassium', 'ptt', 'inr', \n",
    "        'pt', 'sodium', 'bun', 'wbc']  # -hematocrit\n",
    "comobs = ['congestive_heart_failure', 'chronic_pulmonary', 'pulmonary_circulation']\n",
    "others = ['age', 'gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_val(x):\n",
    "    vals = x[~np.isnan(x)]\n",
    "    if len(vals):\n",
    "        return vals[-1]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def featurize_time(df):\n",
    "    out = dict()\n",
    "    for i in range(len(df)):\n",
    "        for lab in labs:\n",
    "            val = last_val(df[lab].values[:i+1])\n",
    "            if lab not in out:\n",
    "                out[lab] = [val]\n",
    "            else:\n",
    "                out[lab].append(val)\n",
    "        for vital in vitals:    \n",
    "            val = last_val(df[vital].values[:i+1])\n",
    "            if vital not in out:\n",
    "                out[vital] = [val]\n",
    "            else:\n",
    "                out[vital].append(val)\n",
    "        for comob in comobs:    \n",
    "            val = last_val(df[comob].values[:i+1])\n",
    "            if comob not in out:\n",
    "                out[comob] = [val]\n",
    "            else:\n",
    "                out[comob].append(val)\n",
    "        for other in others:\n",
    "            val = last_val(df[other].values[:i+1])\n",
    "            if other not in out:\n",
    "                out[other] = [val]\n",
    "            else:\n",
    "                out[other].append(val)\n",
    "        out['timepoint'] = df.timepoint.values\n",
    "        out['label'] = [int(x) for x in df.ventilated.values]\n",
    "    return pd.Series(out)\n",
    "\n",
    "def featurize(df):\n",
    "    out = dict()\n",
    "    for lab in labs:\n",
    "        out[lab] = last_val(df[lab])\n",
    "    for vital in vitals:\n",
    "        out[vital] = last_val(df[vital])\n",
    "    for comob in comobs:\n",
    "        out[comob] = last_val(df[comob])\n",
    "    for other in others:\n",
    "        out[other] = last_val(df[other])\n",
    "    out['label'] = int(df.ventilated.iloc[-1])\n",
    "    return pd.Series(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example from 'original' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['subject_id']==4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['subject_id']==4].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe w/o time (for 'static' models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml = df.set_index(['subject_id', 'timepoint']).groupby(level=0, group_keys=False).\\\n",
    "                                                  apply(featurize).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml[df_ml['subject_id']==4].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe w/ time (for 'timepoint' MAgECs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = df.set_index(['subject_id']).groupby(level=0, group_keys=False).\\\n",
    "                                       apply(featurize_time).apply(pd.Series.explode).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time[df_time['subject_id']==4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time[df_time['subject_id']==4].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "x = df_ml[list(set(df_ml.columns) - {'subject_id', 'label'})]\n",
    "Y = df_ml[['subject_id', 'label']]\n",
    "\n",
    "x_train, x_validation, Y_train, Y_validation = train_test_split(x.copy(), Y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute vitals+labs with mean and co-morbidities with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(df):\n",
    "    df[vitals+labs] = df[vitals+labs].fillna(df[vitals+labs].mean())\n",
    "    df[comobs] = df[comobs].fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = impute(x_train)\n",
    "x_validation = impute(x_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stsc = StandardScaler()\n",
    "xst_train = stsc.fit_transform(x_train)\n",
    "xst_train = pd.DataFrame(xst_train, index=x_train.index, columns=x_train.columns)\n",
    "\n",
    "xst_validation = stsc.transform(x_validation)\n",
    "xst_validation = pd.DataFrame(xst_validation, index=x_validation.index, columns=x_validation.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 'static' models\n",
    "These are single timepoint (single row) models. The training data is grouped by patient and all timepoints are condenced to a single 'last' timepoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def predict(model, data):\n",
    "    \"\"\"\n",
    "    Model output (predicted) probabilities.\n",
    "    Wrapper for predict_proba function in scikit-learn models.\n",
    "    When a model does not have a predict_proba use predict interface.\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probs = model.predict_proba(data)\n",
    "        if probs.shape[1] == 2:\n",
    "            probs = probs[:, 1].ravel()\n",
    "        else:\n",
    "            probs = probs.ravel()\n",
    "    else:\n",
    "        probs = np.array(model.predict(data))\n",
    "    return probs\n",
    "\n",
    "\n",
    "def predict_classes(model, data):\n",
    "    \"\"\"\n",
    "    Model output (predicted) classes.\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'predict_classes'):\n",
    "        return model.predict_classes(data).ravel()\n",
    "    else:\n",
    "         return model.predict(data).ravel()\n",
    "\n",
    "    \n",
    "def evaluate(model, x_test, y_test):\n",
    "    # predict probabilities for test set\n",
    "    yhat_probs = predict(model, x_test)\n",
    "\n",
    "    # predict classes for test set\n",
    "    yhat_classes = predict_classes(model, x_test)\n",
    "    \n",
    "    # reduce to 1d array\n",
    "    if len(yhat_probs[0].shape):\n",
    "        yhat_probs = yhat_probs[:, 0]\n",
    "        yhat_classes = yhat_classes[:, 0]\n",
    " \n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_test, yhat_classes)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, yhat_classes)\n",
    "    print('Precision: %f' % precision)\n",
    "\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, yhat_classes)\n",
    "    print('Recall: %f' % recall)\n",
    "\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_test, yhat_classes)\n",
    "    print('F1 score: %f' % f1)\n",
    "\n",
    "    # ROC AUC\n",
    "    auc = roc_auc_score(y_test, yhat_probs)\n",
    "    print('ROC AUC: %f' % auc)\n",
    "\n",
    "    # confusion matrix\n",
    "    matrix = confusion_matrix(y_test, yhat_classes)\n",
    "    print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(Y_train['label']), Y_train['label'])\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1., class_weight='balanced', solver='lbfgs')\n",
    "lr.fit(xst_train, Y_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(lr, xst_validation, Y_validation['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "rf = CalibratedClassifierCV(RandomForestClassifier(n_estimators=800, \n",
    "                                                   min_samples_split=2, \n",
    "                                                   min_samples_leaf=4, \n",
    "                                                   max_features='sqrt', \n",
    "                                                   max_depth=90, \n",
    "                                                   bootstrap=True, \n",
    "                                                   n_jobs=-1),\n",
    "                            method='sigmoid', cv=5)\n",
    "rf.fit(xst_train, Y_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(rf, xst_validation, Y_validation['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dense(60, input_dim=len(xst_train.columns), activation='relu'))\n",
    "mlp.add(Dropout(0.2))\n",
    "mlp.add(Dense(30, input_dim=60, activation='relu'))\n",
    "mlp.add(Dropout(0.2))\n",
    "mlp.add(Dense(1, activation='sigmoid'))\n",
    "mlp.compile(loss='binary_crossentropy', \n",
    "            loss_weights=[class_weights[1]], optimizer='adam', metrics=['accuracy'])\n",
    "mlp.fit(xst_train, Y_train['label'], epochs=100, batch_size=64, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(mlp, xst_validation, Y_validation['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-aware (LSTM) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/valid\n",
    "train_ind = df_time[~np.isin(df_time['subject_id'], Y_validation.subject_id.unique())].index\n",
    "valid_ind = df_time[np.isin(df_time['subject_id'], Y_validation.subject_id.unique())].index\n",
    "\n",
    "# Impute\n",
    "df_series_train = impute(df_time.iloc[train_ind].copy())\n",
    "df_series_valid = impute(df_time.iloc[valid_ind].copy())\n",
    "\n",
    "# Get X, Y as numpy arrays\n",
    "df_series_train_X = df_series_train[list(set(df_series_train.columns) - \n",
    "                                         {'subject_id', 'label', 'time_point'})].astype(float)\n",
    "\n",
    "df_series_train_Y = df_series_train[['subject_id', 'label', 'time_point']]\n",
    "\n",
    "df_series_valid_X = df_series_valid[list(set(df_series_valid.columns) - \n",
    "                                         {'subject_id', 'label', 'time_point'})].astype(float)\n",
    "\n",
    "df_series_valid_Y = df_series_valid[['subject_id', 'label', 'timepoint']]\n",
    "\n",
    "# scale\n",
    "stsc2 = StandardScaler()\n",
    "tmp = stsc2.fit_transform(df_series_train_X)\n",
    "df_series_train_X = pd.DataFrame(tmp, index=df_series_train_X.index, columns=df_series_train_X.columns)\n",
    "tmp = stsc2.transform(df_series_valid_X)\n",
    "df_series_valid_X = pd.DataFrame(tmp, index=df_series_valid_X.index, columns=df_series_valid_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat X/Y for train/valid\n",
    "df_series_train = pd.concat([df_series_train_X, df_series_train_Y], axis=1)\n",
    "df_series_valid = pd.concat([df_series_valid_X, df_series_valid_Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(df):\n",
    "    x = list()\n",
    "    y = list()    \n",
    "    series_cols = set(df.columns) - {'subject_id', 'timepoint'}    \n",
    "    for ind, fname in df.set_index(['subject_id']).groupby(level=0, group_keys=False):       \n",
    "        y_data = np.array(fname['label'].values[0])\n",
    "        tmp = fname[series_cols].astype(float).values\n",
    "        x_data = np.zeros([25, tmp.shape[1]])\n",
    "        x_data[:tmp.shape[0],:] = tmp\n",
    "        x.append(x_data)\n",
    "        y.append(y_data)    \n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xt_train, Yt_train, xt_valid, Yt_valid = time_series_data(df_time, valid_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(xt_train), len(xt_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(128, dropout=0.5, recurrent_dropout=0.2, input_shape=xt_train.shape[1:]))\n",
    "lstm.add(Dense(1, activation='sigmoid'))\n",
    "lstm.compile(loss='binary_crossentropy',\n",
    "             loss_weights=[class_weights[1]],\n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.fit(xt_train, Yt_train, epochs=100, batch_size=64, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(lstm, xt_valid, Yt_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
